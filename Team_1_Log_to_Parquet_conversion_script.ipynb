{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from multiprocessing import Pool\n",
        "import datetime\n",
        "import pandas as pd\n",
        "\n",
        "# Define the path for processed log files\n",
        "processed_folder_path = '/content/July31'\n",
        "os.makedirs(processed_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path for input log files\n",
        "logs_folder_path = '/content/July31_log_files/July31'\n",
        "\n",
        "# Define the path to store Parquet files\n",
        "parquet_output_path = '/content/Latest_ProcessedFiles'\n",
        "os.makedirs(parquet_output_path, exist_ok=True)\n",
        "\n",
        "# Define a regular expression pattern to extract data from log records\n",
        "log_pattern = r'(\\d{4}-\\d{2}-\\d{2}) (\\d{2}:\\d{2}:\\d{2},\\d{3}) (-\\d{4}) - \\[(\\w+)::(\\w+)\\] - (.+)'\n",
        "\n",
        "def process_log_file(file_path):\n",
        "    # Function to process a single log file and extract relevant information\n",
        "    with open(file_path, 'r', encoding='ISO-8859-1') as log_file:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        gu_id = file_name.split('_')[0]\n",
        "        rows = []\n",
        "        current_log = []  # To store lines of the current log entry\n",
        "        for line in log_file:\n",
        "            match = re.match(log_pattern, line)\n",
        "            if match:\n",
        "                # If a new log entry begins, concatenate and append the previous log entry\n",
        "                if current_log:\n",
        "                    log_entry = ' '.join(current_log)\n",
        "                    date, time, code, message_type, message_name, message = match.groups()\n",
        "                    rows.append([date, time, code, message_type, message_name, log_entry, gu_id])\n",
        "                    current_log = []  # Reset the current log\n",
        "                date, time, code, message_type, message_name, message = match.groups()\n",
        "                rows.append([date, time, code, message_type, message_name, message, gu_id])\n",
        "            else:\n",
        "                # If the line doesn't match the pattern, append it to the previous row's message column with a space\n",
        "                if rows and current_log:\n",
        "                    current_log.append(line.strip())\n",
        "                else:\n",
        "                    current_log = [line.strip()]  # Start a new log entry\n",
        "        return rows\n",
        "\n",
        "log_files = [os.path.join(logs_folder_path, file_name) for file_name in os.listdir(logs_folder_path) if os.path.isfile(os.path.join(logs_folder_path, file_name))]\n",
        "total_log_files = len(log_files)\n",
        "unique_records_per_run = 1  # Number of log files per batch\n",
        "\n",
        "processed_count = 0\n",
        "processed_data = []\n",
        "batch_number = 50  # Initialize the batch number\n",
        "processed_file_names = []  # Initialize a list to store processed log file names\n",
        "\n",
        "with Pool() as pool:\n",
        "    for file_path in log_files:\n",
        "        if processed_count >= total_log_files:\n",
        "            break\n",
        "\n",
        "        if not os.path.exists(os.path.join(processed_folder_path, os.path.basename(file_path))):\n",
        "            try:\n",
        "                results = pool.map(process_log_file, [file_path])\n",
        "                all_results = [item for sublist in results for item in sublist]\n",
        "                processed_data.extend(all_results)\n",
        "                processed_count += 1\n",
        "                os.rename(file_path, os.path.join(processed_folder_path, os.path.basename(file_path)))\n",
        "\n",
        "                if processed_count % unique_records_per_run == 0 or processed_count == total_log_files:\n",
        "                    # Construct the output Parquet file name with the batch number\n",
        "                    output_parquet_file = os.path.join(parquet_output_path, f'July31_Batch_{batch_number}.parquet')\n",
        "\n",
        "                    # Write the Arrow Table to the Parquet file\n",
        "                    df = pd.DataFrame(processed_data, columns=['Date', 'Time', 'Code', 'Message Type', 'Message Name', 'Message', 'GU_ID'])\n",
        "                    table = pa.Table.from_pandas(df)\n",
        "                    pq.write_table(table, output_parquet_file)\n",
        "\n",
        "                    # Append the processed log file names to the list\n",
        "                    processed_file_names.append(os.path.basename(file_path))\n",
        "\n",
        "                    print(f\"Processed {unique_records_per_run} unique log files. Saved as {output_parquet_file}\")\n",
        "\n",
        "                    # Clear the processed data and increment the batch number\n",
        "                    processed_data = []\n",
        "                    batch_number += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {file_path}: {str(e)}\")\n",
        "\n",
        "# After processing all log files, print the list of processed log file names\n",
        "print(f\"Processed log files: {processed_file_names}\")\n",
        "print(f\"Total processed log files: {processed_count}\")\n"
      ],
      "metadata": {
        "id": "MoZlKBy5y-Fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1198430a-63a9-4d2b-9858-8024020520a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed log files: []\n",
            "Total processed log files: 0\n"
          ]
        }
      ]
    }
  ]
}